import pylab as pl
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from pandas.plotting import autocorrelation_plot
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima_model import ARIMA
import warnings
import seaborn as sns
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 12, 6

class ARIMAForecaster:
    '''
    A class to run an Auto-Regressive Integrated Moving Average (ARIMA) forecasting model on 
    the pre-processed data frame generated by feature_engineering.py.
    The following hyperparameters may be tuned by calling the method
    ARIMAForecaster.do_hyperparameter_grid_search():
        - p (lag order): number of lag observations to fit
        - d (degree of differencing): number of times to difference the raw time stream
        - q (order of moving average): size of moving average window
    A walk-forward dynamic forecast is performed by calling the method 
    ARIMAForecaster.walk_forward_forecast(). 
    '''
    
    def __init__(self, dataframe, target_column='mean_transaction_amount', p=3, d=1, q=0):
        self.target_column = target_column
        dates = pd.to_datetime(dataframe.mean_transaction_yearmonth)
        self.timestream = pd.DataFrame({'date': dates,
                                        'subsidy': dataframe[target_column]})
        self.timestream.set_index('date', inplace=True)
        self.p_hyperparameter = p
        self.d_hyperparameter = d
        self.q_hyperparameter = q
    
    # Method to split data into training / test sets:
    
    def form_training_and_test_sets(self, training_fraction=0.66):
        '''
        A function to form training and test sets from the input data.
        By default the fraction of data in the training set (training_fraction)
        is 0.66, with the final third of the time-ordered data resderved for 
        testing via the walk_forward_forecast() function defined below.
        The training and test sets, as well as the split index dividing the two,
        are stored as class attributes.
        '''
        self.split_idx = int(round(len(self.timestream.subsidy.values) * training_fraction))
        self.train = self.timestream.subsidy.values[:self.split_idx]
        self.test = self.timestream.subsidy.values[self.split_idx:]
    
    # Methods to test stationarity / view autocorrelation plots:
    
    def decompose_seasonality(self):
        '''
        A method to plot the raw time stream and its seasonal, trend, and
        residual decomposition.
        '''
        if self.target_column.startswith('log_'):
            ts_log = self.timestream
        else:
            ts_log = np.log(self.timestream.subsidy)
        decomposition = seasonal_decompose(ts_log, freq=1)

        trend = decomposition.trend
        seasonal = decomposition.seasonal
        residual = decomposition.resid

        pl.subplot(411)
        pl.plot(ts_log, label='Original')
        pl.legend(loc='best')
        pl.subplot(412)
        pl.plot(trend, label='Trend')
        pl.legend(loc='best')
        pl.subplot(413)
        pl.plot(seasonal,label='Seasonality')
        pl.legend(loc='best')
        pl.subplot(414)
        pl.plot(residual, label='Residuals')
        pl.legend(loc='best')
        pl.tight_layout(); pl.show()

    def test_differenced_timestream_stationarity(self):
        '''
        A method to difference the raw tine stream and perform the augmented 
        Dickey-Fuller (ADF) test to confirm that the result is stationary.
        Autocorrelation function (ACF) and partial-autocorrelation function (PCF) 
        plots are also produced to assist in determining the optimal autoregression 
        parameter p and moving average parameter q. 
        '''
        differenced = np.diff(self.timestream.subsidy)
        result = adfuller(differenced)
        print 'ADF statistic: %f' % result[0]
        print 'p-value: %f' % result[1]
        print 'critical values:'
        for key, value in result[4].items():
            print '\t%s: %.3f' % (key, value)
        pl.figure()
        pl.subplot(211)
        plot_acf(self.timestream.subsidy, ax=pl.gca())
        pl.subplot(212)
        plot_pacf(self.timestream.subsidy, ax=pl.gca())
        pl.tight_layout()
        pl.savefig('autocorrelation.png')
        pl.show()

    # Hyperparameter grid search methods:

    def evaluate_arima_model(self, data, p, d, q):
        '''
        A helper function for the do_hyperparameter_grid_search()
        method.  Given a time stream ("data"_ and specified p, d, 
        and q hyperparameters, it tests an ARIMA model tuned to the 
        speficied hyperparameters.
        Output: the root mean squared error for the model.
        '''
        split_idx = int(len(data) * 0.66)
        test_set = data[split_idx:]
        train_set = [sample for sample in data[:split_idx]]
        predictions = []
        for i in range(len(test_set)):
            model = ARIMA(train_set, order=(p, d, q))
            model_fitted = model.fit(disp=0)
            prediction = model_fitted.forecast()[0]
            predictions.append(prediction)
            train_set.append(test_set[i])
        mse = mean_squared_error(test_set, predictions)
        rmse = np.sqrt(mse)
        return rmse

    def do_hyperparameter_grid_search(self, verbose=True):
        '''
        A method to perform a hyperparameter grid search to identify the
        optimal p, d, and q hyperparameters for the ARIMA model.  It calls 
        the helper function evaluate_arima_model() to test each model,
        and assigns the optimal hyperparameters to the class attributes 
        self.p_hyperparameter, self.d_hyperparameter, and self.q_hyperparameter.
        '''
        warnings.filterwarnings("ignore")
        p_values = range(0,13)
        d_values = range(0, 4)
        q_values = range(0, 13)
        best_score, best_config = float('inf'), None
        if verbose:
            print 'Starting hyperparameter grid search...'
        for p in p_values:
            for d in d_values:
                for q in q_values:
                    try:
                        rmse = self.evaluate_arima_model(self.train, p, d, q)
                        if rmse < best_score:
                            best_score, best_config = rmse, (p, d, q)
                        if verbose:
                            print('ARIMA%s RMSE=%.3f' % ((p, d, q), rmse))
                    except:
                        continue
        if verbose:
            print('Optimal model: (p ,d, q)=%s; RMSE=%.3f' % (best_config, best_score))
        best_p, best_d, best_q = best_config
        self.p_hyperparameter = best_p
        self.d_hyperparameter = best_d
        self.q_hyperparameter = best_q
    
    # Forecasting methods:
    
    def walk_forward_forecast(self, p, d, q, do_plots=True):
        '''
        A method to perform a walk-forward forecast of the test data set.
        At each step the model produces a forecast of the target variable
        one time sample in the future; records the error = (predicted value 
        minus observed value); and adds the observed value to the training set
        for use in the next forecasting step.
        It then sets the following class attributes:
            - self.predictions: test set predictions generated by the model
            - self.conf_int_lowers: lower bound of the 95% confidence interval
              for the predictions
            - self.conf_int_uppers: upper bound of the 95% confidence interval
              for the predictions
            - self.rmse: the root mean squared error (RMSE) of the model predictions
        '''
        test_set = [x for x in self.train]
        predictions = []
        std_errs = [] 
        conf_int_lowers, conf_int_uppers = [], []
        for i in range(len(self.test)):
            model = ARIMA(test_set, order=(p, d, q))
            model_fitted = model.fit(disp=0)
            prediction, std_err, conf_interval = model_fitted.forecast()
            predictions.append(prediction)
            std_errs.append(std_err)
            conf_int_lowers.append(conf_interval[0][0])
            conf_int_uppers.append(conf_interval[0][1])
            observation = self.test[i]
            test_set.append(observation)
        self.predictions = predictions
        self.conf_int_lowers = conf_int_lowers
        self.conf_int_uppers = conf_int_uppers
        error = mean_squared_error(self.test, predictions)
        self.rmse = np.sqrt(error)

    def plot_forecast(self, plot_confidence_intervals=True, save=False, show=True):
        '''
        A method to plot the model's training and test sets, predictions,
        and 95% confidence interval.
        '''
        xs = pd.to_datetime(self.timestream.index.values)
        rmse = self.rmse
        ylabel = 'Monthly Avg Subsidy (USD)'
        if self.target_column.startswith('log_'):
            mse = mean_squared_error(self.test, np.exp(self.predictions))
            rmse = np.sqrt(mse)
            ylabel = 'log Monthly Avg Subsidy (log USD)'
        pl.plot(xs[:self.split_idx], self.train, 'bo-', alpha=0.5, label='train')
        pl.plot(xs[self.split_idx:], self.test, 'co-', alpha=1.0, label='test')
        pl.plot(xs[self.split_idx:], self.predictions, 'ro-', alpha=0.5, label='prediction')
        if plot_confidence_intervals:
            pl.fill_between(xs[self.split_idx:], self.conf_int_lowers, self.conf_int_uppers, 
                            color='r', alpha=0.2, label='95% prediction interval')
        pl.title('ARIMA Model Forecast: RMSE = %.2f' % rmse)
        pl.legend(loc='best'); pl.xticks(rotation=30., ha='right')
        pl.ylabel(ylabel)
        if save:
            pl.savefig('arima_forecast.png')
        if show:
            pl.show()
    
    def examine_residuals(self):
        '''
        A method to examine the autocorrelation plot and partial autocorrelation plot
        of the model residuals to check them for autocorrelation.
        '''
        residuals = [self.predictions[i] - self.test[i] for i in range(len(self.test))]
        residuals = pd.DataFrame(residuals)
        pl.figure()
        pl.subplot(211)
        plot_acf(residuals, ax=pl.gca())
        pl.subplot(212)
        plot_pacf(residuals, ax=pl.gca())
        pl.tight_layout();
        pl.show()
        
    # Wrapper function:
    
    def run(self, decompose_seasonality=False, test_stationarity=False, do_gridsearch=True):
        '''
        A wrapper method to run the forecasting pipeline.
        '''
        self.form_training_and_test_sets()
        if decompose_seasonality:
            self.decompose_seasonality()
        if test_stationarity:
            self.test_differenced_timestream_stationarity()
        if do_gridsearch:
            self.do_hyperparameter_grid_search()
        self.walk_forward_forecast(self.p_hyperparameter, self.d_hyperparameter,
                                   self.q_hyperparameter)
        self.plot_forecast()
        self.examine_residuals()